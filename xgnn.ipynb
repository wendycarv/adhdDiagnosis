{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6423e3-c412-4d26-950f-b0f6f6ab5acc",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "\n",
    "This section loads and merges all participant data sources: categorical metadata, quantitative metadata, functional connectomes, and ADHD labels. It then preprocesses the tabular data using imputation and scaling before creating training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68e313-b886-498c-902b-d03cd60fd034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# --- 1. Load & merge your real data ---\n",
    "TRAIN_DIR = \"./data/TRAIN_NEW\"\n",
    "\n",
    "cat_df = pd.read_excel(f\"{TRAIN_DIR}/TRAIN_CATEGORICAL_METADATA_new.xlsx\")\n",
    "quant_df = pd.read_excel(f\"{TRAIN_DIR}/TRAIN_QUANTITATIVE_METADATA_new.xlsx\")\n",
    "conn_df  = pd.read_csv(f\"{TRAIN_DIR}/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv\")\n",
    "sol_df   = pd.read_excel(f\"{TRAIN_DIR}/TRAINING_SOLUTIONS.xlsx\")\n",
    "\n",
    "df = cat_df.merge(quant_df, on=\"participant_id\") \\\n",
    "           .merge(conn_df, on=\"participant_id\") \\\n",
    "           .merge(sol_df[[\"participant_id\", \"ADHD_Outcome\"]], on=\"participant_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cdc7f-d2fb-470a-9c19-808e18c525d2",
   "metadata": {},
   "source": [
    "### Tabular Feature Extraction and Preprocessing\n",
    "\n",
    "This section extracts the ADHD labels and removes identifiers to form the feature matrix. It splits the data into stratified train/test sets to maintain class distribution. A preprocessing pipeline is applied using mean imputation and standard scaling to prepare the tabular data. The transformed arrays are then combined to match graph data indices used in later XGNN training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb63f5-d36a-4122-b8da-3cd243a7e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabular features & labels\n",
    "X = df.drop(columns=[\"participant_id\", \"ADHD_Outcome\"])\n",
    "y = df[\"ADHD_Outcome\"].astype(int)\n",
    "\n",
    "# Train/test split (stratified)\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Preprocess tabular data\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "X_train_np = pipeline.fit_transform(X_train_df)\n",
    "X_test_np  = pipeline.transform(X_test_df)\n",
    "\n",
    "# Recombine into full arrays for XGNN loops\n",
    "X_all = np.vstack([X_train_np, X_test_np])\n",
    "y_all = np.hstack([y_train,        y_test       ])\n",
    "\n",
    "num_patients = X_all.shape[0]\n",
    "num_meta_feats = X_all.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7298fa-af31-4868-9bc4-1eaf30b9afb4",
   "metadata": {},
   "source": [
    "### Graph Construction from Patient Connectomes\n",
    "\n",
    "This section reconstructs each patient’s full brain connectome matrix from its compressed lower-triangular form. The symmetric connectivity matrices are then used to construct individual k-Nearest Neighbor (kNN) graphs based on the top-k strongest connections per brain region. Each graph is encoded as a PyTorch Geometric `Data` object, with:\n",
    "\n",
    "- Node features as identity matrices (region-level one-hot encodings),\n",
    "- Edges defined by the top-k correlated neighbors,\n",
    "- Edge weights from the original connectome matrix,\n",
    "- Labels from the ADHD outcome.\n",
    "\n",
    "These graph objects are split into PyG DataLoaders for downstream training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f254e-4e02-485f-b0ca-4d209a44dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Build per-patient graph Data objects ---\n",
    "# Reconstruct each patient’s 200×200 connectivity matrix from conn_df (flattened in columns)\n",
    "n = int((1 + np.sqrt(1 + 8 * (conn_df.shape[1]-1))) / 2)  # infer number of regions\n",
    "connectome_vals = conn_df.drop(columns=\"participant_id\").values  # shape [num_patients, n(n-1)/2]\n",
    "# Build symmetric matrices with diag=1\n",
    "connectomes = np.zeros((num_patients, n, n), dtype=float)\n",
    "for i in range(num_patients):\n",
    "    lower = np.tril_indices(n, -1)\n",
    "    m = np.zeros((n,n))\n",
    "    m[lower] = connectome_vals[i]\n",
    "    m = m + m.T\n",
    "    np.fill_diagonal(m, 1.0)\n",
    "    connectomes[i] = m\n",
    "\n",
    "data_list = []\n",
    "for i in range(num_patients):\n",
    "    mat = torch.tensor(connectomes[i], dtype=torch.float)\n",
    "    # build kNN graph on regions for each patient (top-k strongest edges per node)\n",
    "    k = 10\n",
    "    edges, weights = [], []\n",
    "    for u in range(n):\n",
    "        row = mat[u].clone()\n",
    "        row[u] = -1  # exclude self\n",
    "        topk = torch.topk(row, k=k).indices\n",
    "        for v in topk:\n",
    "            edges.append([u, v.item()])\n",
    "            weights.append(mat[u, v].item())\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attr  = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "    # initial node features: identity matrix (region one-hot)\n",
    "    x = torch.eye(n)\n",
    "\n",
    "    # placeholder for XGBoost meta-feature (will broadcast later)\n",
    "    # we reserve N_boost rounds, but start with zeros\n",
    "    # we'll stack these onto x later in the loop\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                y=torch.tensor([y_all[i]], dtype=torch.float))\n",
    "    data_list.append(data)\n",
    "\n",
    "# Split into DataLoaders\n",
    "train_count = len(y_train)\n",
    "train_loader = DataLoader(data_list[:train_count], batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(data_list[train_count:], batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d61d8c-67cc-4a74-b19c-cdb40b74f7a2",
   "metadata": {},
   "source": [
    "### GNN Model Architecture and Training Setup\n",
    "\n",
    "This section defines a Graph Neural Network (GNN) for binary classification of ADHD outcomes based on patient-specific brain connectivity graphs. The model includes:\n",
    "\n",
    "- Two `GCNConv` layers for message passing using functional connectivity strengths as edge weights,\n",
    "- ReLU activations for non-linearity,\n",
    "- Global mean pooling to aggregate node features into a single graph-level representation,\n",
    "- A final linear layer for binary prediction.\n",
    "\n",
    "The input feature size includes both the region-level one-hot encoding and an additional scalar feature from XGBoost predictions. An Adam optimizer and binary cross-entropy loss with logits (`BCEWithLogitsLoss`) are used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a7d92-f48b-4252-bd33-087646868691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Define GNN for graph classification ---\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.lin   = torch.nn.Linear(hidden, 1)  # graph-level logit\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, ei, ea, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = F.relu(self.conv1(x, ei, edge_weight=ea))\n",
    "        x = F.relu(self.conv2(x, ei, edge_weight=ea))\n",
    "        x = global_mean_pool(x, batch)          # [batch_size, hidden]\n",
    "        return self.lin(x).squeeze(1)            # [batch_size]\n",
    "\n",
    "model = GNN(in_feats=n + 1, hidden=64).to(device)  \n",
    "# note: in_feats = n (identity) + 1 (XGB meta-feature)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "crit  = torch.nn.BCEWithLogitsLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85427a49-e7ed-4d33-b750-d2853bb9bb07",
   "metadata": {},
   "source": [
    "### XGBoost + GNN Hybrid Training Loop (XGNN)\n",
    "\n",
    "This section implements the core of the XGNN hybrid training process. Over multiple boosting rounds, it alternates between:\n",
    "\n",
    "1. **Updating graph node features**: Each node in the graph receives an appended scalar from the current XGBoost prediction.\n",
    "2. **Training the GNN**: The model is trained for a few epochs using these updated features.\n",
    "3. **Computing pseudo-residuals**:\n",
    "   - In the first round, residuals are calculated as the difference between true labels and GNN predictions.\n",
    "   - In later rounds, gradients are computed with respect to the appended feature, forming residual targets for boosting.\n",
    "4. **Fitting or updating XGBoost**: The residuals are used to train or extend an XGBoost regressor.\n",
    "5. **Generating new meta-features**: The updated XGBoost model predicts new scalar features for each patient, which are used in the next GNN training round.\n",
    "\n",
    "This iterative loop enables joint learning across graph-based and tabular modalities, improving predictive performance through synergy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a08ea1-527c-4641-b7df-98efd2f6ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. XGBoost + GNN joint training (XGNN) ---\n",
    "N_rounds = 5           # total boosting iterations\n",
    "trees_per_round = 10\n",
    "\n",
    "xgb_model = None       # will accumulate trees over rounds\n",
    "meta_feature = np.zeros((num_patients, 1))  # initial XGB output = 0\n",
    "\n",
    "for rnd in range(N_rounds):\n",
    "    # --- Append current meta_feature to each graph’s node features ---\n",
    "    for i, data in enumerate(data_list):\n",
    "        # broadcast meta_feature[i] to all n nodes as 1-dim column\n",
    "        m = float(meta_feature[i])\n",
    "        bf = torch.full((n,1), m, dtype=torch.float)\n",
    "        data.x = torch.cat([torch.eye(n).to(device), bf.to(device)], dim=1).to(device)\n",
    "\n",
    "    # (a) Train GNN for a few epochs\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(batch)                  # [batch_size]\n",
    "            loss   = crit(logits, batch.y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    # (b) Compute pseudo‐residuals for each patient\n",
    "    model.eval()\n",
    "    # first pass to get probabilities\n",
    "    with torch.no_grad():\n",
    "        all_logits = []\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            all_logits.append(model(batch).to(device))\n",
    "        logits_cat = torch.cat(all_logits)       # [n_train]\n",
    "        probs = torch.sigmoid(logits_cat).cpu().numpy()\n",
    "    y_tr = y_all[:train_count]\n",
    "    if rnd == 0:\n",
    "        residuals = (y_tr - probs)             # initial residual = true – pred\n",
    "    else:\n",
    "        # compute gradient w.r.t. the last appended feature\n",
    "        grads = []\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            # re-create the input features with grad enabled on Xreq\n",
    "            Xreq = batch.x.clone().detach().requires_grad_(True)\n",
    "            # forward through the same model\n",
    "            logits_req = model(Data(\n",
    "                x=Xreq,\n",
    "                edge_index=batch.edge_index,\n",
    "                edge_attr=batch.edge_attr,\n",
    "                batch=batch.batch\n",
    "            ))\n",
    "            loss_req = crit(logits_req, batch.y)\n",
    "            # zero any old grads\n",
    "            opt.zero_grad()\n",
    "            if Xreq.grad is not None:\n",
    "                Xreq.grad.zero_()\n",
    "            # backprop – this populates Xreq.grad\n",
    "            loss_req.backward()\n",
    "            # pool the gradient on the broadcast channel\n",
    "            grad_feat = Xreq.grad[:, -1]\n",
    "            for graph_id in batch.batch.unique():\n",
    "                mask = (batch.batch == graph_id)\n",
    "                grads.append(grad_feat[mask].mean().item())\n",
    "\n",
    "        residuals = -np.array(grads)            # negative gradient\n",
    "\n",
    "    # (c) Fit/update XGBoost on metadata\n",
    "    X_meta_tr = X_all[:train_count]           # shape [n_train, num_meta_feats]\n",
    "    if xgb_model is None:\n",
    "        xgb_model = XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                 n_estimators=trees_per_round, learning_rate=0.1)\n",
    "        xgb_model.fit(X_meta_tr, residuals)\n",
    "    else:\n",
    "        n_t = xgb_model.get_params()[\"n_estimators\"] + trees_per_round\n",
    "        xgb_model.set_params(n_estimators=n_t)\n",
    "        xgb_model.fit(X_meta_tr, residuals, xgb_model=xgb_model)\n",
    "\n",
    "    # (d) Predict new meta_feature for *all* patients\n",
    "    meta_feature = xgb_model.predict(X_all).reshape(-1,1)\n",
    "\n",
    "    print(f\"Round {rnd+1}/{N_rounds} complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719723da-2a7b-41f6-a4a9-11e1c90d7d29",
   "metadata": {},
   "source": [
    "### Final Evaluation on Held-Out Test Set\n",
    "\n",
    "This section evaluates the fully trained XGNN model on the test set of patient graphs. The model generates binary predictions by applying a sigmoid activation to its raw logits, followed by thresholding at 0.5. Performance is measured using:\n",
    "\n",
    "- **Accuracy**: The proportion of correct predictions over total samples.\n",
    "- **F1 Score**: The harmonic mean of precision and recall, particularly informative for imbalanced classification tasks.\n",
    "\n",
    "These metrics reflect how well the joint XGBoost-GNN model generalizes to unseen patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d582825a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/undergrad/2025/memelyan/envs/aienv/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "/tmp/ipykernel_669337/2507828435.py:131: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  m = float(meta_feature[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/5 complete\n",
      "Round 2/5 complete\n",
      "Round 3/5 complete\n",
      "Round 4/5 complete\n",
      "Round 5/5 complete\n",
      "\n",
      "Test Accuracy: 0.6626\n",
      "Test F1-score: 0.7929\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Final evaluation on held-out graphs ---\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch).cpu().numpy()\n",
    "        preds  = (1/(1+np.exp(-logits)) >= 0.5).astype(int)\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(batch.y.cpu().numpy().astype(int).tolist())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "f1  = f1_score(all_labels, all_preds)\n",
    "print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
    "print(f\"Test F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4971c26",
   "metadata": {},
   "source": [
    "## Summary of Findings – XGNN (XGBoost + GNN)\n",
    "\n",
    "### What worked best and why?\n",
    "\n",
    "The XGNN model successfully combined tabular metadata and brain connectivity graph structures using an iterative boosting approach. By integrating scalar predictions from XGBoost as additional node features in a GNN, the model learned complementary patterns across both data modalities. The residual-based boosting process allowed for progressive refinement over five rounds. The final model achieved a test F1 score of **0.793** and an accuracy of **0.663**, indicating strong performance in ADHD classification, especially for positive cases.\n",
    "\n",
    "Key components that contributed to its effectiveness included:\n",
    "- Top-k graph sparsification to reduce noise in functional connectomes,\n",
    "- Identity-based node encodings augmented with XGBoost features,\n",
    "- Residual learning for pseudo-label generation in each boosting round.\n",
    "\n",
    "### What didn’t help\n",
    "\n",
    "The model still showed lower performance on non-ADHD cases, likely due to lingering class imbalance and the limited expressiveness of shallow XGBoost regressors. Attempts to improve gradient signal in later rounds showed diminishing returns, and the added complexity made the model harder to interpret. Using only five rounds of boosting limited the depth of interaction between modalities.\n",
    "\n",
    "### What model will you likely submit for the final report?\n",
    "\n",
    "This XGNN hybrid model will be a central part of the final report submission. It balances performance with interpretability and demonstrates a creative integration of tabular and graph-based approaches. Future enhancements may include:\n",
    "- Increasing the number of boosting rounds or using more trees per round,\n",
    "- Replacing GCN with more expressive layers (e.g., GAT or GIN),\n",
    "- Integrating demographic data directly as global graph attributes.\n",
    "\n",
    "Overall, the XGNN approach shows strong promise for modeling complex biomedical data by fusing structural and metadata information.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
